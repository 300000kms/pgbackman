#!/usr/bin/env python
#
# Copyright (c) 2013 Rafael Martinez Guerrero (PostgreSQL-es)
# rafael@postgresql.org.es / http://www.postgresql.org.es/
#
# This file is part of PgBackMan
# https://github.com/rafaelma/pgbackman
#
# PgBackMan is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# PgBackMan is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with PgBck.  If not, see <http://www.gnu.org/licenses/>.

import subprocess
import tempfile
import datetime
import sys
import os
import time
import signal
import argparse

from pgbackman.logs import *
from pgbackman.database import * 
from pgbackman.config import *

'''
This program is used by PgBackMan to run backup definitions and snapshots.
'''

global_parameters = {}
backup_server_cache_data = {}
pgsql_node_cache_data = {}


# ############################################
# Function pg_dumpall()
# ############################################
    
def pg_dumpall(db):
    '''Used to take backups with code CLUSTER'''
    
    global global_parameters

    pg_dumpall_command = global_parameters['backup_server_pgsql_bin_dir'] + '/pg_dumpall' + \
        ' -h ' + global_parameters['pgsql_node_fqdn'] + \
        ' -p ' + global_parameters['pgsql_node_port'] + \
        ' -U ' + global_parameters['pgsql_node_admin_user'] + \
        ' --file=' + global_parameters['cluster_dump_file'] + \
        ' ' + global_parameters['extra_backup_parameters']

    try:
        with open(global_parameters['cluster_log_file'],'w') as cluster_log_file:
            
            cluster_log_file.write('------------------------------------\n')
            cluster_log_file.write('Timestamp:' + str(datetime.datetime.now()) + '\n')
            cluster_log_file.write('Command: ' + pg_dumpall_command + '\n')
            cluster_log_file.write('------------------------------------\n\n')

            cluster_log_file.flush()

            proc = subprocess.Popen([pg_dumpall_command],stdout=cluster_log_file,stderr=subprocess.STDOUT,shell=True)
            proc.wait()
                
            if proc.returncode == 0:
                logs.logger.info('Cluster dump file created - %s',global_parameters['cluster_dump_file'])
                cluster_log_file.write('[OK] Cluster dump file created - ' + global_parameters['cluster_dump_file'] + '\n')
                
                global_parameters['execution_status'] = 'SUCCEEDED'
            else:
                logs.logger.critical('Cluster dump file could not be created. Return code = %s. Check log file: %s',proc.returncode,global_parameters['cluster_log_file'])
                cluster_log_file.write('[ERROR] Cluster dump file could not be created. Return code = ' + str(proc.returncode) + '. Check log file: ' + global_parameters['cluster_log_file'] + '\n')
                
                global_parameters['execution_status'] = 'ERROR'
                global_parameters['error_message'] = 'pgdumpall returncode: ' + str(proc.returncode) + '. Check log file.'
                register_backup_catalog(db)
                sys.exit(1)
                
    except Exception as e:
        logs.logger.critical('Could not generate the final cluster dump file %s - %s',global_parameters['cluster_dump_file'],e)
        
        global_parameters['execution_status'] = 'ERROR'
        global_parameters['error_message'] = e
        register_backup_catalog(db)  
        sys.exit(1)
        

# ############################################
# Function pg_dump()
# ############################################
    
def pg_dump(db):
    '''Used to take database backups with codes FULL, SCHEMA and DATA'''

    global global_parameters

    if global_parameters['backup_code'] == 'FULL':
            
        pg_dump_command = global_parameters['backup_server_pgsql_bin_dir'] + '/pg_dump' + \
            ' -h ' + global_parameters['pgsql_node_fqdn'] + \
            ' -p ' + global_parameters['pgsql_node_port'] + \
            ' -U ' + global_parameters['pgsql_node_admin_user'] + \
            ' --file=' + global_parameters['database_dump_file'] + \
            ' --format=c' + \
            ' --blobs' + \
            ' --verbose' + \
            ' ' + global_parameters['extra_backup_parameters'] + \
            ' ' + global_parameters['dbname']         
           
    elif global_parameters['backup_code'] == 'SCHEMA':
        
        pg_dump_command = global_parameters['backup_server_pgsql_bin_dir'] + '/pg_dump' + \
            ' -h ' + global_parameters['pgsql_node_fqdn'] + \
            ' -p ' + global_parameters['pgsql_node_port'] + \
            ' -U ' + global_parameters['pgsql_node_admin_user'] + \
            ' --schema-only' + \
            ' --file=' + global_parameters['database_dump_file'] + \
            ' --format=c' + \
            ' --verbose' + \
            ' ' + global_parameters['extra_backup_parameters'] + \
            ' ' + global_parameters['dbname']
                        
    elif global_parameters['backup_code'] == 'DATA':

        pg_dump_command = global_parameters['backup_server_pgsql_bin_dir'] + '/pg_dump' + \
            ' -h ' + global_parameters['pgsql_node_fqdn'] + \
            ' -p ' + global_parameters['pgsql_node_port'] + \
            ' -U ' + global_parameters['pgsql_node_admin_user'] + \
            ' --data-only' + \
            ' --file=' + global_parameters['database_dump_file'] + \
            ' --format=c' + \
            ' --blobs' + \
            ' --verbose' + \
            ' ' + global_parameters['extra_backup_parameters'] + \
            ' ' + global_parameters['dbname']
        
    try:
        with open(global_parameters['database_log_file'],'w') as database_log_file:
            
            database_log_file.write('------------------------------------\n')
            database_log_file.write('Timestamp:' + str(datetime.datetime.now()) + '\n')
            database_log_file.write('Command: ' + pg_dump_command + '\n')
            database_log_file.write('------------------------------------\n\n')
            
            database_log_file.flush()

            proc = subprocess.Popen([pg_dump_command],stdout=database_log_file,stderr=subprocess.STDOUT,shell=True)
            proc.wait()
                
            if proc.returncode != 0:
                logs.logger.critical('Database dump file could not be created. Return code = %s. Check log file: %s',proc.returncode,global_parameters['database_log_file'])
                
                global_parameters['execution_status'] = 'ERROR'
                global_parameters['error_message'] = 'pg_dump returncode: ' + str(proc.returncode) + '. Check log file.'
                register_backup_catalog(db)  
                sys.exit(1)
                
            logs.logger.info('Database dump file created - %s',global_parameters['database_dump_file'])
            global_parameters['execution_status'] = 'SUCCEEDED'

    except Exception as e:
        logs.logger.critical('Could not generate the final database dump file %s - %s',global_parameters['database_dump_file'],e)

        global_parameters['execution_status'] = 'ERROR'
        global_parameters['error_message'] = e
        register_backup_catalog(db)  
        sys.exit(1)
        

# ############################################
# Function pg_dump_users()
# ############################################
    
def pg_dump_users(db):
    '''Used to backup roles information associated with a database.'''
    
    global global_parameters
    roles = []

    #
    # We need a list with all the roles that own an object in the
    # database we are taking a backup for. We do this in this way:
    #
    # - We generate a temp file with a schema dump of the database
    # - We extract all the roles used in sql statements that use OWNER TO
    # - We extract all roles used in sql statements GRANT .... TO
    # - We generate a temp file with a pg_dumpall -r dump of the cluster running our database
    # - We extract all the lines from the second temp file that have information about some
    #   of the roles extracted from the first temp file
    #
    # We do all this process via temp files insteed of piping the
    # outputs from process to process to avoid allocation problems
    # with huge sql dumps
    #

    # 
    # Extracting roles from the schema dump of the database
    #
    
    try:

        pg_dump_schema_temp_file = tempfile.NamedTemporaryFile(delete=True,dir=global_parameters['tmp_dir'])
        logs.logger.debug('pg_dump schema temp file created %s',pg_dump_schema_temp_file.name)

        pg_dump_command = global_parameters['backup_server_pgsql_bin_dir'] + '/pg_dump' + \
            ' -h ' + global_parameters['pgsql_node_fqdn'] + \
            ' -p ' + global_parameters['pgsql_node_port'] + \
            ' -U ' + global_parameters['pgsql_node_admin_user'] + \
            ' -s' + \
            ' -C' + \
            ' --file=' + pg_dump_schema_temp_file.name + \
            ' --verbose' + \
            ' ' + global_parameters['dbname']

        with open(global_parameters['roles_log_file'],'w') as roles_log_file:
            
            roles_log_file.write('------------------------------------\n')
            roles_log_file.write('Timestamp:' + str(datetime.datetime.now()) + '\n')
            roles_log_file.write('Command: ' + pg_dump_command + '\n')
            roles_log_file.write('------------------------------------\n\n')
            
            roles_log_file.flush()

            proc = subprocess.Popen([pg_dump_command],stdout=roles_log_file,stderr=subprocess.STDOUT,shell=True)
            proc.wait()
        
            if proc.returncode != 0:
                logs.logger.critical('The command used to generate the tmp schema dump of the database %s has a return value != 0',global_parameters['dbname'])

                global_parameters['execution_status'] = 'ERROR'
                global_parameters['error_message'] = 'pg_dump returncode: ' + str(proc.returncode) + '. Check log file.' 
                register_backup_catalog(db)  
                sys.exit(1)
                
            with open(pg_dump_schema_temp_file.name, 'r') as sqldump:
                for line in sqldump:
                    
                    if 'OWNER TO' in line:
                        role = line.split(' OWNER TO ')
                        
                        if len(role) == 2:
                            if role[1].replace(';\n','').find(' ') < 0:
                                roles.append(role[1].replace(';\n',''))
                            
                    if 'GRANT' in line:
                        role = line.split(' TO ')
                        
                        if len(role) == 2:
                            if role[1].replace(';\n','').find(' ') < 0:
                                if role[1].replace(';\n','') != 'PUBLIC':
                                    roles.append(role[1].replace(';\n',''))
                             
            unique_role_list = set(roles)
            logs.logger.debug('The list of roles we need to restore the database %s has been generated - %s',global_parameters['dbname'],unique_role_list)
                        
    except Exception as e:
        logs.logger.critical('pg_dump schema temp file could not be created in directory %s - %s',global_parameters['tmp_dir'],e)
        
        global_parameters['execution_status'] = 'ERROR'
        global_parameters['error_message'] = e
        register_backup_catalog(db)  
        sys.exit(1)
        
    #
    # Extracting sql statements for our roles from the pg_dumpall -r
    # dump of the cluster
    # 
    
    try:
        pg_dumpall_roles_temp_file = tempfile.NamedTemporaryFile(delete=True,dir=global_parameters['tmp_dir'])
        logs.logger.debug('pg_dumpall roles temp file created %s',pg_dumpall_roles_temp_file.name)
    
        pg_dumpall_command = global_parameters['backup_server_pgsql_bin_dir'] + '/pg_dumpall' + \
            ' -r' + \
            ' -h ' + global_parameters['pgsql_node_fqdn'] + \
            ' -p ' + global_parameters['pgsql_node_port'] + \
            ' -U ' + global_parameters['pgsql_node_admin_user'] + \
            ' --file=' + pg_dumpall_roles_temp_file.name

        with open(global_parameters['roles_log_file'],'a') as roles_log_file:

            roles_log_file.write('\n------------------------------------\n')
            roles_log_file.write('Timestamp:' + str(datetime.datetime.now()) + '\n')
            roles_log_file.write('Command: ' + pg_dumpall_command + '\n')
            roles_log_file.write('------------------------------------\n\n')
            
            roles_log_file.flush()
            
            proc = subprocess.Popen([pg_dumpall_command],stdout=roles_log_file,stderr=subprocess.STDOUT,shell=True)
            proc.wait()
        
            if proc.returncode != 0:
                logs.logger.critical('The command used to generate the role dump for the PgSQL node running the database %s has a return value != 0',global_parameters['dbname'])
            
                global_parameters['execution_status'] = 'ERROR'
                global_parameters['error_message'] = 'pg_dumpall returncode: ' + str(proc.returncode) + '. Check log file.'
                register_backup_catalog(db)  
                sys.exit(1)

            with open(global_parameters['roles_dump_file'],'w') as roles_dump_file:

                roles_dump_file.write('-- \n-- PgBackMan \n-- \n-- Roles needed by the database: \n-- ' + global_parameters['dbname'] + '@' + global_parameters['pgsql_node_fqdn'] + ' \n--\n\n')
                roles_dump_file.write('BEGIN;\n\n')

                with open(pg_dumpall_roles_temp_file.name, 'r') as sqldump_roles:
                    for line in sqldump_roles:
                        for role in unique_role_list:
                        
                            #
                            # CREATE ROLE statements
                            #
                            if ' ROLE ' + role + ';' in line:
                                roles_dump_file.write(line)
                            
                            #
                            # ALTER ROLE statements
                            #
                            if ' ROLE ' + role + ' ' in line:
                                roles_dump_file.write(line)
                                
                    logs.logger.debug('The list of role statements we need from pg_dumpall has been generated')

                roles_dump_file.write('\nCOMMIT;\n\n')
                roles_dump_file.write('-- \n-- PgBackMan roles dump completed\n--\n\n')
                    
            logs.logger.info('Final roles dump file created - %s',global_parameters['roles_dump_file'])
            global_parameters['execution_status'] = 'SUCCEEDED'
            global_parameters['role_list'] = list(unique_role_list)
            
    except Exception as e:
        logs.logger.critical('Could not generate the final roles dump file %s - %s',global_parameters['roles_dump_file'],e)
        
        global_parameters['execution_status'] = 'ERROR'
        global_parameters['error_message'] = e
        register_backup_catalog(db)  
        sys.exit(1)
        

# ############################################
# Function pg_dump_database_config()
# ############################################
    
def pg_dump_database_config(db):
    '''Used to backup database configuration information associated with a database'''

    global global_parameters

    #
    # Extracting sql statements for our database from the pg_dumpall
    # -s dump of the cluster
    # 
    # We extract all the CREATE DATABASE, ALTER DATABASE and
    # GRANT/REVOKE ... ON DATABASE statements for our database
    #
    
    try:
        pg_dumpall_dbconfig_temp_file = tempfile.NamedTemporaryFile(delete=True,dir=global_parameters['tmp_dir'])
        logs.logger.debug('pg_dumpall dbconfig temp file created %s',pg_dumpall_dbconfig_temp_file.name)
    
        pg_dumpall_command = global_parameters['backup_server_pgsql_bin_dir'] + '/pg_dumpall' + \
            ' -s' + \
            ' -h ' + global_parameters['pgsql_node_fqdn'] + \
            ' -p ' + global_parameters['pgsql_node_port'] + \
            ' -U ' + global_parameters['pgsql_node_admin_user'] + \
            ' --file=' + pg_dumpall_dbconfig_temp_file.name

        with open(global_parameters['dbconfig_log_file'],'w') as dbconfig_log_file:

            dbconfig_log_file.write('\n------------------------------------\n')
            dbconfig_log_file.write('Timestamp:' + str(datetime.datetime.now()) + '\n')
            dbconfig_log_file.write('Command: ' + pg_dumpall_command + '\n')
            dbconfig_log_file.write('------------------------------------\n\n')
            
            dbconfig_log_file.flush()
        
            proc = subprocess.Popen([pg_dumpall_command], stdout=dbconfig_log_file,stderr=subprocess.STDOUT,shell=True)
            proc.wait()

            if proc.returncode != 0:
                logs.logger.critical('The command used to generate the dbconfig dump for the PgSQL node running the database %s has a return value != 0',global_parameters['dbname'])
            
                global_parameters['execution_status'] = 'ERROR'
                global_parameters['error_message'] = 'pg_dumpall returncode: ' + str(proc.returncode) + '. Check log file.'
                register_backup_catalog(db)  
                sys.exit(1)
            
            with open(global_parameters['dbconfig_dump_file'],'w') as dbconfig_dump_file:

                dbconfig_dump_file.write('-- \n-- PgBackMan \n-- \n-- Database attributes needed by the database: \n-- ' + global_parameters['dbname'] + '@' + global_parameters['pgsql_node_fqdn'] + ' \n--\n\n')
                dbconfig_dump_file.write('BEGIN;\n\n')

                with open(pg_dumpall_dbconfig_temp_file.name, 'r') as sqldump_dbconfig:
                    for line in sqldump_dbconfig:

                        #
                        # CREATE DATABASE
                        #
                        if 'CREATE DATABASE ' + global_parameters['dbname'] + ' ' in line:
                            dbconfig_dump_file.write(line)
                                                         
                        #
                        # ALTER DATABASE statements
                        #
                        if 'ALTER DATABASE ' + global_parameters['dbname'] + ' SET ' in line:
                            dbconfig_dump_file.write(line)
            
                        #
                        # GRANT/REVOKE ... ON DATABASE statements
                        #

                        if ' ON DATABASE ' + global_parameters['dbname'] + ' ' in line:
                            dbconfig_dump_file.write(line)
                                                      
                logs.logger.debug('The list of dbconfig statements we need from pg_dumpall has been generated')

                dbconfig_dump_file.write('\nCOMMIT;\n\n')
                dbconfig_dump_file.write('-- \n-- PgBackMan dbconfig dump completed\n--\n\n')
                    
            logs.logger.info('Final dbconfig dump file created - %s',global_parameters['dbconfig_dump_file'])
            global_parameters['execution_status'] = 'SUCCEEDED'
                
    except Exception as e:
        logs.logger.critical('Could not generate the final dbconfig dump file %s - %s',global_parameters['dbconfig_dump_file'],e)
        
        global_parameters['execution_status'] = 'ERROR'
        global_parameters['error_message'] = e
        register_backup_catalog(db)  
        sys.exit(1)


# ############################################
# Function get_pgsql_node_dsn()
# ############################################
    
def get_pgsql_node_dsn():
    '''Get the DSN values needed to connect to a PgSQL node'''

    global global_parameters

    dsn_value = 'host=' + global_parameters['pgsql_node_fqdn'] + ' port=' + global_parameters['pgsql_node_port'] + ' dbname=' + global_parameters['dbname'] + ' user=' + global_parameters['pgsql_node_admin_user']

    logs.logger.debug('DSN value for PgSQL node %s is %s',global_parameters['pgsql_node_fqdn'],dsn_value)
    return dsn_value
    

# ############################################
# Function get_pgsql_node_release()
# ############################################
    
def get_pgsql_node_release(db,db_pgnode):
    '''Get the postgreSQL release version a PgSQL node is running'''
    
    pgsql_node_version = ''

    try:
        db_pgnode.pg_connect()
        pgsql_node_version = str(db_pgnode.get_server_version())[0:3]
        db_pgnode.pg_close()
    except Exception as e:
        logs.logger.critical('Problems getting the postgreSQL version running on Pgsql node - %s',e)
             
    logs.logger.debug('PgSQl node version: %s',pgsql_node_version)

    if pgsql_node_version == '904':
        pgsql_node_release = '9_4'
    elif pgsql_node_version == '903':
        pgsql_node_release = '9_3'
    elif pgsql_node_version == '902':
        pgsql_node_release = '9_2'
    elif pgsql_node_version == '901':
        pgsql_node_release = '9_1'
    elif pgsql_node_version == '900':
        pgsql_node_release = '9_0'
    else:
        pgsql_node_release = None

    if  pgsql_node_release != None:
        logs.logger.debug('PgSQL node %s is running postgreSQL %s',global_parameters['pgsql_node_fqdn'],pgsql_node_release)
        return pgsql_node_release
    else:
        logs.logger.critical('Could not get the postgreSQL release for this PgSQL node: %s',global_parameters['pgsql_node_fqdn'])
        global_parameters['execution_status'] = 'ERROR'
        global_parameters['error_message'] = 'Problems getting pgsql release'
        register_backup_catalog(db)

        sys.exit(1)
    
    
# ############################################
# Function get_backup_server_pgsql_bin_dir()
# ############################################
    
def get_backup_server_pgsql_bin_dir(db):
    '''Get the directory with postgreSQL binaries to use'''

    global global_parameters

    try:
        pgsql_bin_dir = db.get_backup_server_parameter(global_parameters['backup_server_id'],'pgsql_bin_' + global_parameters['pgsql_node_release'])
        logs.logger.debug('pgsql bin directory to use: %s',pgsql_bin_dir)
        
        return pgsql_bin_dir

    except Exception as e:

        pgsql_bin_dir = backup_server_cache_data['pgsql_bin_' + global_parameters['pgsql_node_release']]
        logs.logger.debug('pgsql bin directory to use: %s',pgsql_bin_dir)
        
        return pgsql_bin_dir
        

# ############################################
# Function get_filename_id()
# ############################################
    
def get_filename_id(dump_type,file_type):
    '''Generate the filename used for the backup and log files of a backup job'''
    
    global global_parameters

    timestamp = datetime.datetime.now().strftime('%Y%m%dT%H%M%S')

    #
    # Dump generated by an AT job
    #

    if global_parameters['def_id'] != None and global_parameters['snapshot_id'] == None:

        if dump_type == 'CLUSTER':
            filename_id = global_parameters['pgsql_node_backup_dir'] + '/' + file_type + '/' + dump_type + '-' + global_parameters['pgsql_node_fqdn'] + '-v' + global_parameters['pgsql_node_release'] + '-defid' + global_parameters['def_id'] + '-c' + global_parameters['backup_code'] + '-' + timestamp
        else:
            filename_id = global_parameters['pgsql_node_backup_dir'] + '/' + file_type + '/' + global_parameters['dbname'] + '-' + global_parameters['pgsql_node_fqdn'] + '-v' + global_parameters['pgsql_node_release'] + '-defid' + global_parameters['def_id'] + '-c' + global_parameters['backup_code'] + timestamp + '-' + dump_type 

    #
    # Dump generated by a CRON job
    #

    elif global_parameters['snapshot_id'] != None and global_parameters['def_id'] == None:

        if dump_type == 'CLUSTER':
            filename_id = global_parameters['pgsql_node_backup_dir'] + '/' + file_type + '/' + dump_type + '-' + global_parameters['pgsql_node_fqdn'] + '-v' + global_parameters['pgsql_node_release'] + '-snapid' + global_parameters['snapshot_id'] + '-c' + global_parameters['backup_code'] + '-' + timestamp
        else:
            filename_id = global_parameters['pgsql_node_backup_dir'] + '/' + file_type + '/' + global_parameters['dbname'] + '-' + global_parameters['pgsql_node_fqdn'] + '-v' + global_parameters['pgsql_node_release'] + '-snapid' + global_parameters['snapshot_id'] + '-c' + global_parameters['backup_code'] + timestamp + '-' + dump_type 


    return filename_id



# ############################################
# Function register_backup_catalog()
# ############################################
  
def register_backup_catalog(db):
    '''Update the backup catalog information in the database'''

    global global_parameters

    global_parameters['backup_stop'] = datetime.datetime.now()
    duration = global_parameters['backup_stop'] - global_parameters['backup_start']

    #
    # Execution method information
    #

    if global_parameters['def_id'] == None:
        global_parameters['execution_method'] = 'AT'
        
    elif global_parameters['snapshot_id'] == None:
        global_parameters['execution_method'] = 'CRON'

    #
    # pg_dump_file information
    #
    
    if os.path.exists(global_parameters['cluster_dump_file']):
        pg_dump_file = global_parameters['cluster_dump_file']
        pg_dump_log_file = global_parameters['cluster_log_file']

        try:
            pg_dump_file_size = os.path.getsize(global_parameters['cluster_dump_file'])
        except OSError as e:
            logs.logger.error('Could not get size of pg_dump_file: %s - %s',global_parameters['cluster_dump_file'],e)

    elif os.path.exists(global_parameters['database_dump_file']):
        pg_dump_file = global_parameters['database_dump_file']
        pg_dump_log_file = global_parameters['database_log_file']

        try:
            pg_dump_file_size = os.path.getsize(global_parameters['database_dump_file'])
        except OSError as e:
            logs.logger.error('Could not get size of pg_dump_file: %s - %s',global_parameters['database_dump_file'],e)

    else:
        pg_dump_file = 'None'
        pg_dump_file_size = 0
    
        if os.path.exists(global_parameters['cluster_log_file']):
            pg_dump_log_file = global_parameters['cluster_log_file']
        elif os.path.exists(global_parameters['database_log_file']):
            pg_dump_log_file = global_parameters['database_log_file']
        else:
            pg_dump_log_file = 'None'

    #
    # pg_dump_roles_file information
    #

    if os.path.exists(global_parameters['roles_dump_file']):
        pg_dump_roles_file = global_parameters['roles_dump_file']
        pg_dump_roles_log_file = global_parameters['roles_log_file']

        try:
            pg_dump_roles_file_size = os.path.getsize(global_parameters['roles_dump_file'])
        except OSError as e:
            logs.logger.error('Could not get size of pg_dump roles file: %s - %s',global_parameters['roles_dump_file'],e)
    else:
        pg_dump_roles_file = 'None'
        pg_dump_roles_file_size = 0
        
        if os.path.exists(global_parameters['roles_log_file']):
            pg_dump_roles_log_file = global_parameters['roles_log_file']
        else:
            pg_dump_roles_log_file = 'None'

    #
    # pg_dump_dbconfig_file information
    #

    if os.path.exists(global_parameters['dbconfig_dump_file']):
        pg_dump_dbconfig_file = global_parameters['dbconfig_dump_file']
        pg_dump_dbconfig_log_file = global_parameters['dbconfig_log_file']

        try:
            pg_dump_dbconfig_file_size = os.path.getsize(global_parameters['dbconfig_dump_file'])
        except OSError as e:
            logs.logger.error('Could not get size of pg_dump dbconfig file: %s - %s',global_parameters['dbconfig_dump_file'],e)
    else:
        pg_dump_dbconfig_file = 'None'
        pg_dump_dbconfig_file_size = 0
        
        if os.path.exists(global_parameters['dbconfig_log_file']):
            pg_dump_dbconfig_log_file = global_parameters['dbconfig_log_file']
        else:
            pg_dump_dbconfig_log_file = 'None'

    #
    # Updating database
    #

    try:
        procpid = os.getpid()
        db.register_backup_catalog(global_parameters['def_id'],
                                       procpid,
                                       global_parameters['backup_server_id'],  
                                       global_parameters['pgsql_node_id'],
                                       global_parameters['dbname'],
                                       global_parameters['backup_start'],
                                       global_parameters['backup_stop'],
                                       duration,
                                       pg_dump_file,
                                       pg_dump_file_size,
                                       pg_dump_log_file,
                                       pg_dump_roles_file,
                                       pg_dump_roles_file_size,
                                       pg_dump_roles_log_file,
                                       pg_dump_dbconfig_file,
                                       pg_dump_dbconfig_file_size,
                                       pg_dump_dbconfig_log_file,
                                       global_parameters['global_log_file'],
                                       global_parameters['execution_status'],
                                       global_parameters['execution_method'],
                                       global_parameters['error_message'],
                                       global_parameters['snapshot_id'],
                                       global_parameters['role_list'],
                                       global_parameters['pgsql_node_release'].replace('_','.')
                                       )
    

        logs.logger.info('Backup job catalog for DefID: %s or SnapshotID: %s updated in the database',str(global_parameters['def_id']),str(global_parameters['snapshot_id']))
        
    except Exception as e:

        #
        # We create a pending log file if we can not update the
        # database.  This file will be processed by
        # pgbackman_maintenence later.
        # 

        logs.logger.warning('Problems updating the backup job catalog for DefID: %s or SnapshotID: %s in the database - %s',str(global_parameters['def_id']),str(global_parameters['snapshot_id']),e)

        pending_log_file = ''
        
        try:
            procpid = os.getpid()
            pending_log_file = global_parameters['backup_server_pending_registration_dir'] + '/backup_jobs_pending_log_updates_nodeid_' + str(global_parameters['pgsql_node_id']) + '_' + str(procpid) + '.log'

            if global_parameters['def_id'] == None:
                global_parameters['def_id'] = ''
        
            elif global_parameters['snapshot_id'] == None:
                global_parameters['snapshot_id'] = ''
                
            with open(pending_log_file,'w+') as catalog_pending:
                catalog_pending.write(str(global_parameters['def_id']) + '::' +
                                      str(procpid) + '::' +
                                      str(global_parameters['backup_server_id']) + '::' +   
                                      str(global_parameters['pgsql_node_id']) + '::' +   
                                      global_parameters['dbname'] + '::' +   
                                      str(global_parameters['backup_start']) + '::' +   
                                      str(global_parameters['backup_stop']) + '::' +   
                                      str(duration) + '::' +   
                                      pg_dump_file + '::' +   
                                      str(pg_dump_file_size) + '::' +   
                                      pg_dump_log_file + '::' +   
                                      pg_dump_roles_file + '::' +   
                                      str(pg_dump_roles_file_size) + '::' +   
                                      pg_dump_roles_log_file + '::' +   
                                      pg_dump_dbconfig_file + '::' +   
                                      str(pg_dump_dbconfig_file_size) + '::' +   
                                      pg_dump_dbconfig_log_file + '::' +   
                                      global_parameters['global_log_file'] + '::' + 
                                      global_parameters['execution_status'] + '::' +
                                      global_parameters['execution_method'] + '::' +
                                      global_parameters['error_message'] + '::' +
                                      global_parameters['snapshot_id'] + '::' + 
                                      ' '.join(global_parameters['role_list']) + '::' +
                                      global_parameters['pgsql_node_release'].replace('_','.') +
                                      '\n')
                
                logs.logger.info('Catalog pending log file: %s created',pending_log_file)
        
        except Exception as e:
            logs.logger.error('Could not generate the catalog pending log file: %s - %s',pending_log_file,e)


# ##################################################
# Function get_backup_server_parameters_from_cache()
# ##################################################

def get_backup_server_parameters_from_cache(db,backup_server_fqdn):  
    '''Get backup server parameters from cache file'''

    global backup_server_cache_data
    global global_parameters

    try:
        backup_server_cache_file = global_parameters['root_backup_dir'] + '/cache_dir/backup_server_' + backup_server_fqdn + '.cache'
        
        with open(backup_server_cache_file,'r') as backup_server_cache:
            for line in backup_server_cache:
                (key, val) = line.split('::')
                backup_server_cache_data[key] = val.replace('\n','')
                
    except Exception as e:
        logs.logger.error('Could not read the cache file for the backup server: %s - %s',backup_server_fqdn,e)
        global_parameters['execution_status'] = 'ERROR'
        global_parameters['error_message'] = 'Problems getting backup server cache data'
        register_backup_catalog(db)

        sys.exit(1)


# ###############################################
# Function get_pgsql_node_parameters_from_cache()
# ###############################################

def get_pgsql_node_parameters_from_cache(db):  
    '''Get pgsql_node parameters from cache file'''

    global pgsql_node_cache_data

    try:
        pgsql_node_cache_file = global_parameters['root_backup_dir'] + '/cache_dir/pgsql_node_' + global_parameters['pgsql_node_id'] + '.cache'


        with open(pgsql_node_cache_file,'r') as pgsql_node_cache:
            for line in pgsql_node_cache:
                (key, val) = line.split('::')
                pgsql_node_cache_data[key] = val.replace('\n','')
                    
    except Exception as e:
        logs.logger.critical('Could not read the cache file for the PgSQL node: %s - %s',global_parameters['pgsql_node_fqdn'],e)
        global_parameters['execution_status'] = 'ERROR'
        global_parameters['error_message'] = 'Problems getting pgsql node cache data'
        register_backup_catalog(db)

        sys.exit(1)


# ##############################################
# Function check_pgbackman_database_connection()
# ##############################################
  
def check_pgbackman_database_connection(db):
    '''Check if we can connect to the pgbackman database'''

    try:
        db.pg_connect()
        return True
    except Exception as e: 
        logs.logger.error('The pgbackman database is not available - %s',e)
        return False

# ###############################################
# Function check_pgsql_node_database_connection()
# ###############################################
  
def check_pgsql_node_database_connection(db_pgnode):
    '''Check if we can connect to the pgsql node database'''

    try:
        db_pgnode.pg_connect()
        return True
    except Exception as e: 
        logs.logger.critical('The %s database is not available - %s',global_parameters['dbname'],e)
        return False


# ############################################
# Function signal_handler()
# ############################################
    
def signal_handler(signum, frame):
    logs.logger.info('**** pgbackman_dump stopped. ****')
    sys.exit(0)


# ############################################
# Function Main()
# ############################################
    
def main():
    '''Main function'''
    
    global global_parameters

    conf = configuration()
    pgbackman_dsn = conf.dsn

    global_parameters['tmp_dir'] = conf.tmp_dir
    global_parameters['global_log_file'] = conf.log_file
    global_parameters['backup_start'] = datetime.datetime.now()

    global_parameters['cluster_dump_file'] = ''
    global_parameters['cluster_log_file'] = ''

    global_parameters['database_dump_file'] = ''
    global_parameters['database_log_file'] = ''

    global_parameters['roles_dump_file'] = ''
    global_parameters['roles_log_file'] = ''
 
    global_parameters['dbconfig_dump_file'] = ''
    global_parameters['dbconfig_log_file'] = ''

    global_parameters['error_message'] = ''
    global_parameters['role_list'] = [] 

    global_parameters['pgsql_node_release'] = ''
       
    db = pgbackman_db(pgbackman_dsn,'pgbackman_dump')
 
    pgsql_node_dsn = get_pgsql_node_dsn()
    db_pgnode = pgbackman_db(pgsql_node_dsn,'pgbackman_dump')

    # 
    # The backup server FQDN to be used can be defined in the
    # pgbackman configuration file.  If the configuration parameter
    # 'backup_server' is not defined, the return value of
    # socket.getfqdn() will be used.
    #
    # The FQDN of the backup server will be used to find out the
    # internal pgbackman ID of the backup server
    #

    if conf.backup_server != '':
        backup_server_fqdn = conf.backup_server
    else:
        backup_server_fqdn = socket.getfqdn()

    try:
        backup_server_id = db.get_backup_server_id(backup_server_fqdn)
        logs.logger.info('Backup server: %s is registered in pgbackman',backup_server_fqdn)

    except psycopg2.Error as e:
        get_backup_server_parameters_from_cache(db,backup_server_fqdn)
        backup_server_id = backup_server_cache_data['backup_server_id']
        logs.logger.info('Backup server: %s is registered in the cache',backup_server_fqdn)

    global_parameters['backup_server_id'] = backup_server_id

    global_parameters['backup_server_pending_registration_dir'] = global_parameters['root_backup_dir'] + '/pending_updates'
    global_parameters['backup_server_cache_dir'] =  global_parameters['root_backup_dir'] + '/cache_dir'

    get_pgsql_node_parameters_from_cache(db)

    #
    # We check before starting if the database we are going to backup
    # is available.  If it is not available, we will stop the backup
    # job with an error
    #
    # pgbackman_dump has to work and take the backups defined in the
    # system even if the pgbackman database is not available. If the
    # pgbackman database is not available, pgbackman_dump will get the
    # necessary parameters from the cache files saved in the backup
    # server
    #
    
    check_pgnode_db = check_pgsql_node_database_connection(db_pgnode)

    if not check_pgnode_db:
        logs.logger.critical('The database "%s" is not available at %s. Shutting down the backup job with DefID: %s',
                             global_parameters['dbname'],
                             global_parameters['pgsql_node_fqdn'],
                             str(global_parameters['def_id']))
        
        global_parameters['execution_status'] = 'ERROR'
        global_parameters['error_message'] = 'Database to backup not available'
        register_backup_catalog(db)
        sys.exit(1) 

    try:
        global_parameters['pgsql_node_backup_dir'] = db.get_pgsql_node_parameter(global_parameters['pgsql_node_id'],'pgnode_backup_partition')

    except psycopg2.Error as e:
        global_parameters['pgsql_node_backup_dir'] = pgsql_node_cache_data['pgnode_backup_partition']
    
    global_parameters['pgsql_node_release'] = get_pgsql_node_release(db,db_pgnode)
    global_parameters['backup_server_pgsql_bin_dir'] = get_backup_server_pgsql_bin_dir(db)
    
    global_parameters['cluster_dump_file'] = get_filename_id('CLUSTER','dump') + '.sql'
    global_parameters['cluster_log_file'] = get_filename_id('CLUSTER','log') + '.log'

    global_parameters['database_dump_file'] = get_filename_id('DATABASE','dump') + '.sql'
    global_parameters['database_log_file'] = get_filename_id('DATABASE','log') + '.log'

    global_parameters['roles_dump_file'] = get_filename_id('USERS','dump') + '.sql'
    global_parameters['roles_log_file'] = get_filename_id('USERS','log') + '.log'
 
    global_parameters['dbconfig_dump_file'] = get_filename_id('DBCONFIG','dump') + '.sql'
    global_parameters['dbconfig_log_file'] = get_filename_id('DBCONFIG','log') + '.log'
       
    if global_parameters['backup_code'] == 'CLUSTER':
        pg_dumpall(db)
    else:
        pg_dump(db)
        pg_dump_users(db)
        pg_dump_database_config(db)

    register_backup_catalog(db)


# ############################################
# 
# ############################################

if __name__ == '__main__':

    logs = logs("pgbackman_dump")

    signal.signal(signal.SIGINT,signal_handler)
    signal.signal(signal.SIGTERM,signal_handler)

    parser = argparse.ArgumentParser(prog=sys.argv[0])
    parser.add_argument('--node-fqdn', metavar='PGSQL-NODE-FQDN', required=True, help='PgSQL node FQDN', dest='pgsql_node_fqdn')
    parser.add_argument('--node-id', metavar='PGSQL-ID', required=True, help='PgSQL node ID', dest='pgsql_node_id')
    parser.add_argument('--node-port', metavar='PGSQL-NODE-PORT', required=True, help='PgSQL node port', dest='pgsql_node_port')
    parser.add_argument('--node-user', metavar='PGSQL-NODE_ADMIN-USER', required=True, help='PgSQL node admin user', dest='pgsql_node_admin_user')
    parser.add_argument('--def-id', metavar='JOBID', required=False, help='Backup job ID', dest='def_id')
    parser.add_argument('--snapshot-id', metavar='SNAPSHOT-ID', required=False, help='snapshot ID', dest='snapshot_id')
    parser.add_argument('--dbname', metavar='DBNAME', required=False, help='Database name', dest='dbname')
    parser.add_argument('--backup-code', metavar='[FULL|SCHEMA|DATA|CLUSTER]', choices=['FULL', 'SCHEMA', 'DATA', 'CLUSTER'], required=True, help='Backup code', dest='backup_code')
    parser.add_argument('--encryption', metavar='[false|true]', default=True, choices=['true', 'false'],required=True, help='Activate encryption', dest='encryption')
    parser.add_argument('--root-backup-dir', metavar='ROOT-BACKUP-DIR', default=True, required=True, help='Root backup dir', dest='root_backup_dir')
    parser.add_argument('--extra-backup-parameters', metavar='EXTRA-PARAMETERS', required=False, help='extra pg_dump parameters', dest='extra_backup_parameters')
    
    args = parser.parse_args()    
    
    logs.logger.info('**** pgbackman_dump started. ****')
    
    if args.pgsql_node_fqdn:
        global_parameters['pgsql_node_fqdn'] = args.pgsql_node_fqdn
    else:
        logs.logger.error('PgSQL node fqdn parameter not defined')
        sys.exit(1)
        
    if args.pgsql_node_id:
        global_parameters['pgsql_node_id'] = args.pgsql_node_id
    else:
        logs.logger.error('PgSQL node id parameter not defined')
        sys.exit(1)
        
    if args.pgsql_node_port:
        global_parameters['pgsql_node_port'] = args.pgsql_node_port
    else:
        logs.logger.error('PgSQL node port parameter not defined')
        sys.exit(1)

    if args.pgsql_node_admin_user:
        global_parameters['pgsql_node_admin_user'] = args.pgsql_node_admin_user
    else:
        logs.logger.error('PgSQL node admin user parameter not defined')
        sys.exit(1)
        
    if args.def_id:
        global_parameters['def_id'] = args.def_id
    else:
        global_parameters['def_id'] = None

    if args.snapshot_id:
        global_parameters['snapshot_id'] = args.snapshot_id
    else:
        global_parameters['snapshot_id'] = None

    if args.dbname:
        global_parameters['dbname'] = args.dbname
    else:
        global_parameters['dbname'] = 'template1'

    if args.encryption:
        global_parameters['encryption'] = args.encryption
    else:
        logs.logger.error('Encryption parameter not defined')
        sys.exit(1)
        
    if args.backup_code:
        global_parameters['backup_code'] = args.backup_code
    else:
        logs.logger.error('Backup code parameter not defined')
        sys.exit(1)

    if args.root_backup_dir:
        global_parameters['root_backup_dir'] = args.root_backup_dir
    else:
        logs.logger.error('Root backup directory parameter not defined')
        sys.exit(1)

    if args.extra_backup_parameters:
        global_parameters['extra_backup_parameters'] = args.extra_backup_parameters.replace("'","")
    else:
        global_parameters['extra_backup_parameters'] = ''

    main()
