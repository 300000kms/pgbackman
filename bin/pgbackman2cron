#!/usr/bin/env python
#
# Copyright (c) 2013 Rafael Martinez Guerrero (PostgreSQL-es)
# rafael@postgresql.org.es / http://www.postgresql.org.es/
#
# This file is part of Pgbackman
# https://github.com/rafaelma/pgbackman
#
# PgBackMan is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# PgBackMan is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Pgbackman.  If not, see <http://www.gnu.org/licenses/>.

'''
This program is used by pgbackman to administrate the crontab files for all the
PgSQL nodes running backup jobs in this Backup server. These crontab files will run pgbackman_dump 
for the backup jobs running in this backup server.

pgbackman2cron uses LISTEN/NOTIFY to track changes on PgSQL nodes using this backup server.

In addition pgbackman2cron will create cache data for the Backup server and all PgSQL nodes. 

pgbackman2cron will catch up with changes that happened during a down period of the program. 
No changes will be lose if pgbackman2cron is down.
'''

import sys
import os
import time
import socket
import signal

from pgbackman.logs import *
from pgbackman.database import *
from pgbackman.config import *

listen_list = []


# ############################################
# Function add_to_listen_channels()
# ############################################

def add_to_listen_channels(db,db_notify,backup_server_id):
    """LISTEN to all active channels"""
    
    global listen_list
    
    old_listen_list = listen_list
    new_listen_list = []
    listen_list = []

    try:
        for channel in db_notify.get_listen_channel_names(backup_server_id): 
            listen_list.append(channel)

        new_listen_list = set(listen_list) - set(old_listen_list)    

        for channel in new_listen_list:
            db_notify.add_listen(channel)

            #
            # Create cache data, backup directory and crontab file 
            # for a pgsql_node if they do not exist. 
            #
            
            if channel != 'channel_pgsql_nodes_updated':
                pgsql_node_id = get_pgsql_node_id_from_channel(channel)
                
                update_pgsql_node_cache_data(db,backup_server_id,pgsql_node_id)
                create_pgsql_node_backup_directories(db,pgsql_node_id)
                
                crontab_file = db.get_pgsql_node_parameter(pgsql_node_id,'pg_node_cron_file')
                
                if not os.path.exists(crontab_file):
                    generate_crontab_backup_jobs(db,backup_server_id,pgsql_node_id)

            logs.logger.info('Listening to channel: %s',channel)

    except Exception as e:
        logs.logger.error('Problems listening to channels - %s',e)
            

# ############################################
# Function delete_from_listen_channels()
# ############################################

def delete_from_listen_channels(db_notify,backup_server_id):
    """UNLISTEN to channels"""
    
    new_listen_list = []
    delete_listen_list = []

    try:
        for channel in db_notify.get_listen_channel_names(backup_server_id): 
            new_listen_list.append(channel)
        
        delete_listen_list = set(listen_list)-set(new_listen_list)
        
        for channel in delete_listen_list:
            db_notify.delete_listen(channel)
        
            logs.logger.info('Unlistening to channel: %s',channel)
 
    except Exception as e:
        logs.logger.error('Problems unlistening to channels - %s',e)

                 
# ############################################
# Function get_pgsql_node_id_from_channel()
# ############################################

def get_pgsql_node_id_from_channel(channel):
    '''Extract PgSQL node ID from channel name'''

    pgsql_id = channel.split('_pg')[1]
    return pgsql_id

            
# ############################################
# Function generate_crontab_backup_jobs()
# ############################################

def generate_crontab_backup_jobs(db,backup_server_id,pgsql_node_id):
    """Generate a crontab file for a PgSQL node"""

    try:
        crontab_file = db.get_pgsql_node_parameter(pgsql_node_id,'pg_node_cron_file')
               
        with open(crontab_file,'w') as file:
            data = db.generate_crontab_backup_jobs(backup_server_id,pgsql_node_id)
            
            file.write(data)
            file.close()
            
            logs.logger.info('Crontab file: %s created/updated',crontab_file)
            
    except Exception as e:
            
        # If we cannot create the crontab file, we have to update
        # the job_queue in the database so we don't loose this update.
            
        try:
            db.update_job_queue(backup_server_id,pgsql_node_id)
            logs.logger.error('Problems creating/updating the crontab file: %s - %s',crontab_file,e)
            
        except Exception as e:
            logs.logger.error('Problems updating job queue for SrvID: %s and nodeID: %s after a crontab file update error - %s',backup_server_id,pgsql_node_id,e)


# ############################################
# Function generate_all_crontab_jobs()
# ############################################
                         
def generate_all_crontab_jobs(db,backup_server_id):
    """Get all the PgSQL node IDs that need to get a new crontab file installed"""
    
    try:
        pgsql_node_id = db.get_next_crontab_id_to_generate(backup_server_id)
    
        while pgsql_node_id != None:
            generate_crontab_backup_jobs(db,backup_server_id,pgsql_node_id)
            pgsql_node_id = db.get_next_crontab_id_to_generate(backup_server_id)
 
        logs.logger.info('All crontab jobs in queue processed')
   
    except Exception as e:
        logs.logger.error('Problems getting next crontab file ID to generate - %s',e)
   

# ############################################
# Function create_global_directories()
# ############################################

def create_global_directories(db,backup_server_fqdn,backup_server_id):
    '''Create global directories used for cache and pending database registrations'''
    
    try:
        root_backup_partition = db.get_backup_server_parameter(backup_server_id,'root_backup_partition')

    except Exception as e:
        logs.logger.error('Problems getting root backup partition used in %s - %s',backup_server_fqdn,e)    

    backup_server_pending_registration_dir = root_backup_partition + '/pending_updates'
    backup_server_cache_dir = root_backup_partition +  '/cache_dir'

    if os.path.exists(backup_server_pending_registration_dir):
        logs.logger.debug('Pending log registration directory exists: %s',backup_server_pending_registration_dir)
    else:
        logs.logger.debug('Pending log registration directory does not exist: %s',backup_server_pending_registration_dir)
        
        try:
            os.makedirs(backup_server_pending_registration_dir,0700)
            logs.logger.info('Pending log registration directory created: %s',backup_server_pending_registration_dir)
        except OSError as e:
            logs.logger.critical('OS error when creating the pending log registration directory: %s',e)
            sys.exit(1)
            
    if os.path.exists(backup_server_cache_dir):
        logs.logger.debug('Cache directory exists: %s',backup_server_cache_dir)
    else:
        logs.logger.debug('Cache directory does not exist: %s',backup_server_cache_dir)
        
        try:
            os.makedirs(backup_server_cache_dir,0700)
            logs.logger.info('Cache directory created: %s',backup_server_cache_dir)

        except OSError as e:
            logs.logger.critical('OS error when creating the cache directory: %s',e)
            sys.exit(1)
            

# ############################################
# Function create_pgsql_node_backup_directories()
# ############################################
    
def create_pgsql_node_backup_directories(db,pgsql_node_id):
    '''
    Create the directories needed for PgSQL nodes backups
    '''

    try:
        pgnode_backup_partition = db.get_pgsql_node_parameter(pgsql_node_id,'pgnode_backup_partition')

    except Exception as e:
        logs.logger.error('Problems getting backup partition used for NodeID: %s - %s',pgsql_node_id,e)   

    if os.path.exists(pgnode_backup_partition + '/dump'):
        logs.logger.debug('Dump directory %s exists',pgnode_backup_partition + '/dump')
    else:
        logs.logger.warning('Dump directory %s does not exist',pgnode_backup_partition + '/dump')
        
        try:
            os.makedirs(pgnode_backup_partition + '/dump',0700)
            logs.logger.info('Dump directory %s created',pgnode_backup_partition + '/dump')
        except OSError as e:
            logs.logger.critical('OS error when creating the dump directory: %s',e)
            sys.exit(1)
                
    if os.path.exists(pgnode_backup_partition + '/log'):
        logs.logger.debug('Log directory %s exists',pgnode_backup_partition + '/log')
    else:
        logs.logger.warning('Log directory %s does not exist',pgnode_backup_partition + '/log')
        
        try:
            os.makedirs(pgnode_backup_partition + '/log',0700)
            logs.logger.info('Log directory %s created',pgnode_backup_partition + '/log')
        except OSError as e:
            logs.logger.critical('OS error when creating the log directory: %s',e)
            sys.exit(1)       


# ############################################
# Function update_backup_server_cache_data()
# ############################################

def update_backup_server_cache_data(db,backup_server_fqdn,backup_server_id):
    '''Update the cache data for the backup server'''

    root_backup_partition = pgsql_bin_9_3 = pgsql_bin_9_2 = pgsql_bin_9_1 = pgsql_bin_9_0 = pgsql_bin_8_4 = None
    
    try:
        root_backup_partition = db.get_backup_server_parameter(backup_server_id,'root_backup_partition')

        pgsql_bin_9_3 = db.get_backup_server_parameter(backup_server_id,'pgsql_bin_9_3')
        pgsql_bin_9_2 = db.get_backup_server_parameter(backup_server_id,'pgsql_bin_9_2')
        pgsql_bin_9_1 = db.get_backup_server_parameter(backup_server_id,'pgsql_bin_9_1')
        pgsql_bin_9_0 = db.get_backup_server_parameter(backup_server_id,'pgsql_bin_9_0')
        pgsql_bin_8_4 = db.get_backup_server_parameter(backup_server_id,'pgsql_bin_8_4')
        
        backup_server_cache_dir = root_backup_partition +  '/cache_dir'
        backup_server_cache_file = backup_server_cache_dir + '/backup_server_' + backup_server_fqdn + '.cache'

        if os.path.exists(backup_server_cache_dir):
        
            with open(backup_server_cache_file,'w') as backup_server_cache:
                backup_server_cache.write('backup_server_id::' + str(backup_server_id) + '\n' +
                                          'backup_server_fqdn::' + backup_server_fqdn + '\n' +
                                          'root_backup_partition::' + root_backup_partition + '\n' +
                                          'pgsql_bin_9_3::' + pgsql_bin_9_3 + '\n' +
                                          'pgsql_bin_9_2::' + pgsql_bin_9_2 + '\n' +
                                          'pgsql_bin_9_1::' + pgsql_bin_9_1 + '\n' +
                                          'pgsql_bin_9_0::' + pgsql_bin_9_0 + '\n' +
                                          'pgsql_bin_8_4::' + pgsql_bin_8_4 + '\n')
                
                logs.logger.info('Cache file: %s created/updated',backup_server_cache_file)
                
    except Exception as e:
        logs.logger.error('Problems updating backup server cache data for %s - %s',backup_server_fqdn,e)   
        
    
# ############################################
# Function update_pgsql_node_cache_data()
# ############################################

def update_pgsql_node_cache_data(db,backup_server_id,pgsql_node_id):
    '''Update the cache data for the backup server'''

    try:
        pgsql_node_fqdn = db.get_pgsql_node_fqdn(pgsql_node_id)
        pgsql_node_backup_dir = db.get_pgsql_node_parameter(pgsql_node_id,'pgnode_backup_partition')
        root_backup_partition = db.get_backup_server_parameter(backup_server_id,'root_backup_partition')
           
        backup_server_cache_dir = root_backup_partition +  '/cache_dir'
        pgsql_node_cache_file = backup_server_cache_dir + '/pgsql_node_' + str(pgsql_node_id) + '.cache'
        
        if os.path.exists(backup_server_cache_dir):
        
            with open(pgsql_node_cache_file,'w') as pgsql_node_cache:
                pgsql_node_cache.write('pgsql_node_id::' + str(pgsql_node_id) + '\n' +
                                       'pgsql_node_fqdn::' + pgsql_node_fqdn + '\n' +
                                       'pgnode_backup_partition::' + pgsql_node_backup_dir + '\n')
                
                logs.logger.info('Cache file: %s created/updated',pgsql_node_cache_file)
                   
    except Exception as e:
        logs.logger.error('Problems updating pgsql node cache data for %s - %s',pgsql_node_fqdn,e)   
   

# ############################################
# Function check_database_connection()
# ############################################
  
def check_database_connection(db):
    '''Check if we can connect to the database server and the pgbackman database'''

    try:
        db.pg_connect()
        return True
    except Exception as e:    
        return False
        

# ############################################
# Function main()
# ############################################

def main():
    global listen_list

    conf = configuration()
    dsn = conf.dsn

    logs.logger.debug('Backup server ID from config file: %s',conf.backup_server)
    logs.logger.debug('Backup server FQDN: %s',socket.getfqdn())
    logs.logger.debug('DSN: host=%s hostaddr=%s port=%s database=%s user=%s ',conf.dbhost,conf.dbhostaddr,conf.dbport,conf.dbname,conf.dbuser)
    logs.logger.debug('Channels check interval: %s',conf.channels_check_interval)

    db = pgbackman_db(dsn,'pgbackman2cron')

    #
    # We check before starting if the database is available. 
    # If it is not available we will wait conf.pg_connect_retry_interval 
    # and try again 

    check_db = check_database_connection(db)

    while not check_db:
        logs.logger.critical('The pgbackman database is not available. Waiting %s seconds before trying again',conf.pg_connect_retry_interval)
        
        time.sleep(conf.pg_connect_retry_interval)
        check_db = check_database_connection(db)
        
    logs.logger.debug('Database server is up and running and pgbackman database is available')
    
    db_notify = pgbackman_db(dsn,'pgbackman_notify')
    
    try:
        db_notify.pg_connect()
    except Exception as e:
        logs.logger.critical('Problems creating a permanent connection to the pgbackman database for LISTEN/NOTIFY data - %s',e)   
        sys.exit(1)

    #
    # Checking if this backup server is registered in pgbackman
    #

    if conf.backup_server != '':
        backup_server_fqdn = conf.backup_server

        try:
            backup_server_id = db.get_backup_server_id(backup_server_fqdn)
            logs.logger.info('Backup server: %s is registered in pgbackman',backup_server_fqdn)

        except psycopg2.Error as e:
            logs.logger.critical('Cannot find backup server %s in pgbackman. Stopping pgbackman2cron.',backup_server_fqdn)
            logs.logger.info('**** PgBackMan2cron stopped. ****')
            sys.exit()   
    else:
        backup_server_fqdn = socket.getfqdn()
    
        try:
            backup_server_id = db.get_backup_server_id(backup_server_fqdn)
            logs.logger.info('Backup server: %s is registered in pgbackman',backup_server_fqdn)

        except psycopg2.Error as e:
            logs.logger.critical('Cannot find backup server %s in pgbackman. Stopping pgbackman2cron.',backup_server_fqdn)
            logs.logger.info('**** PgBackMan2cron stopped. ****')
            sys.exit()     

    create_global_directories(db,backup_server_fqdn,backup_server_id)
    update_backup_server_cache_data(db,backup_server_fqdn,backup_server_id)
    
    add_to_listen_channels(db,db_notify,backup_server_id)

    # We check if there are some crontab files to generate when
    # we start pgbackman2cron. This is nesessary just in case 
    # pgbackman2cron has been down and missed some NOTIFYs 
    # from the central database.

    generate_all_crontab_jobs(db,backup_server_id)
 
    #
    # Main loop waiting for notifications
    #
    while True:
        channels = []        
 
        try:
            db_notify.conn.poll()

            while db_notify.conn.notifies:
                channel = db_notify.conn.notifies.pop().channel
                channels.append(channel)

            for channel in set(channels):
                if channel == 'channel_pgsql_nodes_updated':
                    
                    #
                    # A PgSQL node has been registered or deleted
                    #
                    delete_from_listen_channels(db_notify,backup_server_id)
                    add_to_listen_channels(db,db_notify,backup_server_id)
                
                else:
                    
                    #
                    # A backup job has been registered or deleted
                    #
                    pgsql_node_id = db.get_next_crontab_id_to_generate(backup_server_id)
                    generate_crontab_backup_jobs(db,backup_server_id,pgsql_node_id)

            # We check and receive all new notifications after waiting conf.channels_check_interval.
            time.sleep(conf.channels_check_interval)

        except psycopg2.OperationalError as e:

            #
            # If we lose the connection to the database, we will wait conf.pg_connect_retry_interval
            # before trying to connect again. When the database is available again we will reset
            # all the listen channels and check if there are some crontab jobs in queue to be processed
            #

            logs.logger.critical('Operational error: %s',e)

            check_db = check_database_connection(db)
            
            while not check_db:
                logs.logger.critical('We have lost the connection to the database. Waiting %s seconds before trying again',conf.pg_connect_retry_interval)
                
                time.sleep(conf.pg_connect_retry_interval)
                check_db = check_database_connection(db)

            db_notify = None
            db_notify = pgbackman_db(dsn,'pgbackman_notify')
            
            try:
                db_notify.pg_connect()
            except Exception as e:
                logs.logger.critical('Problems creating a permanent connection to the pgbackman database for LISTEN/NOTIFY data - %s',e)   
                sys.exit(1)

            listen_list = []
 
            add_to_listen_channels(db,db_notify,backup_server_id)
            generate_all_crontab_jobs(db,backup_server_id)
    
        except Exception as e:
            logs.logger.error('General error in main loop - %s',e)   
        
    db_notify.pg_close()
    db.pg_close()

        
# ############################################
# Function signal_handler()
# ############################################
    
def signal_handler(signum, frame):
    logs.logger.info('**** PgBackMan2cron stopped. ****')
    sys.exit(0)


# ############################################
# 
# ############################################

if __name__ == '__main__':

    logs = logs("pgbackman2cron")
    logs.logger.info('**** PgBackMan2cron started. ****')

    signal.signal(signal.SIGINT,signal_handler)
    signal.signal(signal.SIGTERM,signal_handler)

    main()

