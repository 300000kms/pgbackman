#!/usr/bin/env python
#
# Copyright (c) 2013 Rafael Martinez Guerrero (PostgreSQL-es)
# rafael@postgresql.org.es / http://www.postgresql.org.es/
#
# This file is part of Pgbackman
# https://github.com/rafaelma/pgbackman
#
# PgBackMan is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# PgBackMan is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Pgbackman.  If not, see <http://www.gnu.org/licenses/>.

import sys
import os
import time
import socket
import signal

sys.path.append('/home/rafael/Devel/GIT/pgbackman')

from pgbackman.logs import *
from pgbackman.database import *
from pgbackman.database_notify import *
from pgbackman.config import *
 
__version__ = "1.0.0"

listen_list = []

# ############################################
# Function
# ############################################

def add_to_listen_channels(db,backup_server_id):
    """Listen to all active channels"""
    
    global listen_list
    
    old_listen_list = listen_list
    new_listen_list = []
    listen_list = []
        
    for channel in db.get_listen_channel_names(backup_server_id): 
        listen_list.append(channel)

    new_listen_list = set(listen_list) - set(old_listen_list)    

    for channel in new_listen_list:
        db.add_listen(channel)
        
        logs.logger.info('Listening to channel: %s',channel)

            
# ############################################
# Function
# ############################################

def delete_from_listen_channels(db,backup_server_id):
    """Unlisten a channel"""
    
    new_listen_list = []
    delete_listen_list = []

    for channel in db.get_listen_channel_names(backup_server_id): 
        new_listen_list.append(channel)
        
        delete_listen_list = set(listen_list)-set(new_listen_list)
        
    for channel in delete_listen_list:
        db.delete_listen(channel)
        
        logs.logger.info('Unlistening to channel: %s',channel)


# ############################################
# Function
# ############################################

def generate_crontab_backup_jobs(db,backup_server_id,pgsql_node_id):
    """Generate a crontab file for a PgSQL node"""

    update_pgsql_node_cache_data(db,backup_server_id,pgsql_node_id)
    create_pgsql_node_backup_directories(db,pgsql_node_id)

    crontab_file = db.get_pgsql_node_parameter(pgsql_node_id,'pg_node_cron_file')

    if os.path.exists(os.path.dirname(crontab_file)):
        
        try:
            file = open(crontab_file,'w',1)
                       
            if file:
                data = db.generate_crontab_backup_jobs(backup_server_id,pgsql_node_id)
                file.write(data)
                file.close()

                logs.logger.info('Crontab file: %s created/updated',crontab_file)
        except IOError as e:
            
            # If we cannot create the crontab file, we have to update
            # the job_queue in the database so we don't loose this update.
            
            db.update_job_queue(backup_server_id,pgsql_node_id)
            logs.logger.error('I/O error when creating/updating a crontab file: %s',e)


# ############################################
# Function
# ############################################
                         
def generate_all_crontab_jobs(db,backup_server_id):
    """Get all the PgSQL node IDs that need to get a new crontab file installed"""
    
    pgsql_node_id = db.get_next_crontab_id_to_generate(backup_server_id)
    
    while pgsql_node_id != None:
        generate_crontab_backup_jobs(db,backup_server_id,pgsql_node_id)
        pgsql_node_id = db.get_next_crontab_id_to_generate(backup_server_id)
 
    logs.logger.info('All crontab jobs in queue processed')


# ############################################
# Function
# ############################################

def create_global_directories(db,backup_server_fqdn,backup_server_id):
    '''Create global directories used for cache and pending database registrations'''
    
    root_backup_partition = db.get_backup_server_parameter(backup_server_id,'root_backup_partition')
    backup_server_pending_registration_dir = root_backup_partition + '/pending_updates'
    backup_server_cache_dir = root_backup_partition +  '/cache_dir'

    if os.path.exists(backup_server_pending_registration_dir):
        logs.logger.debug('Pending log registration directory exists: %s',backup_server_pending_registration_dir)
    else:
        logs.logger.debug('Pending log registration directory does not exist: %s',backup_server_pending_registration_dir)
        
        try:
            os.makedirs(backup_server_pending_registration_dir,0700)
            logs.logger.info('Pending log registration directory created: %s',backup_server_pending_registration_dir)
        except OSError as e:
            logs.logger.critical('OS error when creating the pending log registration directory: %s',e)
            sys.exit(1)
            
    if os.path.exists(backup_server_cache_dir):
        logs.logger.debug('Cache directory exists: %s',backup_server_cache_dir)
    else:
        logs.logger.debug('Cache directory does not exist: %s',backup_server_cache_dir)
        
        try:
            os.makedirs(backup_server_cache_dir,0700)
            logs.logger.info('Cache directory created: %s',backup_server_cache_dir)

            update_backup_server_cache_data(db,backup_server_fqdn,backup_server_id)
        except OSError as e:
            logs.logger.critical('OS error when creating the cache directory: %s',e)
            sys.exit(1)
            
# ############################################
# Function handler
# ############################################
    
def create_pgsql_node_backup_directories(db,pgsql_node_id):
    '''
    Create the directories needed for PgSQL nodes backups
    '''
        
    pgnode_backup_partition = db.get_pgsql_node_parameter(pgsql_node_id,'pgnode_backup_partition')

    if os.path.exists(pgnode_backup_partition + '/dump'):
        logs.logger.debug('Dump directory %s exists',pgnode_backup_partition + '/dump')
    else:
        logs.logger.error('Dump directory %s does not exist',pgnode_backup_partition + '/dump')
        
        try:
            os.makedirs(pgnode_backup_partition + '/dump',0700)
            logs.logger.info('Dump directory %s created',pgnode_backup_partition + '/dump')
        except OSError as e:
            logs.logger.critical('OS error when creating the dump directory: %s',e)
            sys.exit(1)
                
    if os.path.exists(pgnode_backup_partition + '/log'):
        logs.logger.debug('Log directory %s exists',pgnode_backup_partition + '/log')
    else:
        logs.logger.error('Log directory %s does not exist',pgnode_backup_partition + '/log')
        
        try:
            os.makedirs(pgnode_backup_partition + '/log',0700)
            logs.logger.info('Log directory %s created',pgnode_backup_partition + '/log')
        except OSError as e:
            logs.logger.critical('OS error when creating the log directory: %s',e)
            sys.exit(1)       


# ############################################
# Function
# ############################################

def update_backup_server_cache_data(db,backup_server_fqdn,backup_server_id):
    '''Update the cache data for the backup server'''

    root_backup_partition = db.get_backup_server_parameter(backup_server_id,'root_backup_partition')
    pgsql_bin_9_3 = db.get_backup_server_parameter(backup_server_id,'pgsql_bin_9.3')
    pgsql_bin_9_2 = db.get_backup_server_parameter(backup_server_id,'pgsql_bin_9.2')
    pgsql_bin_9_1 = db.get_backup_server_parameter(backup_server_id,'pgsql_bin_9.1')
    pgsql_bin_9_0 = db.get_backup_server_parameter(backup_server_id,'pgsql_bin_9.0')
    pgsql_bin_8_4 = db.get_backup_server_parameter(backup_server_id,'pgsql_bin_8.4')

    backup_server_cache_dir = root_backup_partition +  '/cache_dir'

    backup_server_cache_file = backup_server_cache_dir + '/backup_server_' + backup_server_fqdn + '.cache'

    if os.path.exists(backup_server_cache_dir):
        
        try:
            with open(backup_server_cache_file,'w+') as backup_server_cache:
                backup_server_cache.write(str(backup_server_id) + ',' +
                                          backup_server_fqdn + ',' +
                                          root_backup_partition + ',' +
                                          pgsql_bin_9_3 + ',' +
                                          pgsql_bin_9_2 + ',' +
                                          pgsql_bin_9_1 + ',' +
                                          pgsql_bin_9_0 + ',' +
                                          pgsql_bin_8_4 + ',' +
                                          '\n')
                
                logs.logger.info('Cache file: %s created/updated',backup_server_cache_file)
                
        except IOError as e:
            logs.logger.error('Could not create the cache file for the backup server: [%s] %s - %s',backup_server_id,backup_server_fqdn,e)


# ############################################
# Function
# ############################################

def update_pgsql_node_cache_data(db,backup_server_id,pgsql_node_id):
    '''Update the cache data for the backup server'''

    pgsql_node_fqdn = db.get_pgsql_node_fqdn(pgsql_node_id)
    pgsql_node_backup_dir = db.get_pgsql_node_parameter(pgsql_node_id,'pgnode_backup_partition')
    root_backup_partition = db.get_backup_server_parameter(backup_server_id,'root_backup_partition')
    backup_server_cache_dir = root_backup_partition +  '/cache_dir'

    pgsql_node_cache_file = backup_server_cache_dir + '/pgsql_node_' + str(pgsql_node_id) + '.cache'

    if os.path.exists(backup_server_cache_dir):
        
        try:
            with open(pgsql_node_cache_file,'w+') as pgsql_node_cache:
                pgsql_node_cache.write(str(pgsql_node_id) + ',' +
                                          pgsql_node_fqdn + ',' +
                                          pgsql_node_backup_dir + '\n')
                
                logs.logger.info('Cache file: %s created/updated',pgsql_node_cache_file)
                
        except IOError as e:
            logs.logger.error('Could not create the cache file for the PgSQL node: [%s] %s - %s',pgsql_node_id,pgsql_node_fqdn,e)



# ############################################
# Function
# ############################################
  
def check_database_connection(db):
    '''Check if we can connect to the database server and the pgbackman database'''

    try:
        db.pg_connect()
        return True
    except Exception as e:    
        return False
        

# ############################################
# Function
# ############################################

def main():
    global listen_list

    conf = configuration()
    dsn = conf.dsn

    logs.logger.debug('Backup server ID from config file: %s',conf.backup_server)
    logs.logger.debug('Backup server FQDN: %s',socket.getfqdn())
    logs.logger.debug('DSN: %s',conf.dsn)
    logs.logger.debug('Channels check interval: %s',conf.channels_check_interval)

    db = pgbackman_db(dsn,logs,'pgbackman2cron')

    #
    # We check before starting if the database is available. 
    # If it is not available we will wait conf.pg_connect_retry_interval 
    # and try again 

    check_db = check_database_connection(db)

    while not check_db:
        logs.logger.critical('The pgbackman database is not available. Waiting %s seconds before trying again',conf.pg_connect_retry_interval)
        
        time.sleep(conf.pg_connect_retry_interval)
        check_db = check_database_connection(db)
        
    logs.logger.debug('Database server is up and running and pgbackman database is available')
    db_notify = pgbackman_db_notify(dsn,logs,'pgbackman_notify')
    
    if conf.backup_server != '':
        backup_server_fqdn = conf.backup_server
        backup_server_id = db.get_backup_server_id(backup_server_fqdn)
                
        if backup_server_id == False:
            logs.logger.critical('Backup server %s does not exist in pgbackman. Stopping pgbackman2cron.',conf.backup_server)
            logs.logger.info('**** PgBackMan2cron stopped. ****')
            sys.exit()     
        else:
            logs.logger.info('Backup server: %s up and running',conf.backup_server)
    else:
        backup_server_fqdn = socket.getfqdn()
        backup_server_id = db.get_backup_server_id(backup_server_fqdn)

        if backup_server_id == False:
            logs.logger.critical('Backup server %s does not exist in pgbackman. Stopping pgbackman2cron.',backup_server_fqdn)
            logs.logger.info('**** PgBackMan2cron stopped. ****')
            sys.exit()     
        else:
            logs.logger.info('Backup server: %s exists in pgbackman',backup_server_fqdn)

    create_global_directories(db,backup_server_fqdn,backup_server_id)
    add_to_listen_channels(db_notify,backup_server_id)

    # We check if there are some crontab files to generate when
    # we start pgbackman2cron. This is nesessary just in case 
    # pgbackman2cron has been down and missed some NOTIFYs 
    # from the central database.

    generate_all_crontab_jobs(db,backup_server_id)
 
    #
    # Main loop waiting for notifications
    #
    while True:
        channels = []        
 
        try:
            db_notify.conn.poll()

            while db_notify.conn.notifies:
                channel = db_notify.conn.notifies.pop().channel
                channels.append(channel)

            for channel in set(channels):
                if channel == 'channel_pgsql_nodes_updated':
                    delete_from_listen_channels(db_notify,backup_server_id)
                    add_to_listen_channels(db_notify,backup_server_id)
                else:
                    pgsql_node_id = db.get_next_crontab_id_to_generate(backup_server_id)
                    generate_crontab_backup_jobs(db,backup_server_id,pgsql_node_id)

            # We check and receive all new notifications after waiting conf.channels_check_interval.
            time.sleep(conf.channels_check_interval)
            print 'main LOOP' # debug

        except psycopg2.OperationalError as e:

            #
            # If we lose the connection to the database, we will wait conf.pg_connect_retry_interval
            # before trying to connect again. When the database is available again we will reset
            # all the listen channels and check if there are some crontab jobs in queue to be processed
            #

            logs.logger.critical('Operational error: %s',e)

            check_db = check_database_connection(db)
            
            while not check_db:
                logs.logger.critical('We have lost the connection to the database. Waiting %s seconds before trying again',conf.pg_connect_retry_interval)
                
                time.sleep(conf.pg_connect_retry_interval)
                check_db = check_database_connection(db)

            db_notify = None
            db_notify = pgbackman_db_notify(dsn,logs,'pgbackman_notify')
            
            listen_list = []
 
            add_to_listen_channels(db_notify,backup_server_id)
            generate_all_crontab_jobs(db,backup_server_id)
    
    db_notify.pg_close()
    db.pg_close()
        
# ############################################
# Function handler
# ############################################
    
def signal_handler(signum, frame):
    logs.logger.info('**** PgBackMan2cron stopped. ****')
    sys.exit(0)



# ############################################
# 
# ############################################

if __name__ == '__main__':

    logs = logs("pgbackman2cron")
    logs.logger.info('**** PgBackMan2cron started. ****')

    signal.signal(signal.SIGINT,signal_handler)
    signal.signal(signal.SIGTERM,signal_handler)

    main()


